# BUG-001: GitHub Integration Critical Issues

**Created**: 2025-10-04
**Status**: Investigation Complete
**Priority**: P0 (Blocks GitHub PR reviews)
**Component**: GitHub MCP Integration

---

## Executive Summary

Two critical issues prevent successful GitHub PR reviews:
1. **BUG-001-A**: File content retrieval returns SHA messages instead of actual file content
2. **BUG-001-B**: Claude API rate limits cause all sub-agents to fail on large PRs (>30 files)

**Impact**: 0 findings returned for GitHub PR #1 (55 files, 11,420 additions)

---

## BUG-001-A: GitHub MCP File Content Retrieval Failure

### Status
🔴 **CONFIRMED** - Root cause identified

### Severity
**High** - Blocks code analysis (no file content = no review findings)

### Symptoms
```
MCP response appears to be plain text: successfully downloaded text file (SHA: 21a4bf33f...)
Failed to parse content for .claude/agents/test-analyzer.md: successfully downloaded text file (SHA: 21a4bf33f...)
```

### Root Cause Analysis

#### Problem
Our `parseMCPResponse()` in `app/mcp-utils.js:124-174` expects file content in `content[0].text` or `content[0].data` fields. However, GitHub MCP Server returns a **multi-part response** with:

1. **Part 1** (text type): Status message `"successfully downloaded text file"`
2. **Part 2** (resource type): Actual file content in a `resource` object

#### Current Implementation (Incorrect)
```javascript
// app/mcp-utils.js:124-145
export function parseMCPResponse(response) {
  const content = response.content[0];  // ❌ Only reads FIRST element
  const rawData = content.text || content.data;

  if (typeof rawData === 'string') {
    if (!rawData.trim().startsWith('{')) {
      // ❌ Returns error for "successfully downloaded..." message
      return { error: rawData, isPlainText: true };
    }
  }
}
```

#### Expected GitHub MCP Response Format
```json
{
  "content": [
    {
      "type": "text",
      "text": "successfully downloaded text file"
    },
    {
      "type": "resource",
      "resource": {
        "uri": "repo://owner/repo/contents/path/file.js",
        "mimeType": "text/plain; charset=utf-8",
        "text": "// Actual file content here\nconst foo = 'bar';\n"
      }
    }
  ]
}
```

#### Evidence
- **Source**: Web search found Cursor forum post documenting identical issue
- **Forum Link**: https://forum.cursor.com/t/github-mcp-get-file-contents-tool-fails-to-parse-the-response/111075
- **Confirmation**: GitHub MCP uses JSON-RPC 2.0 format with multi-part `content` arrays

#### Comparison with GitLab MCP (Working Baseline)
```javascript
// GitLab MCP returns single-element content array with JSON string
{
  "content": [{
    "type": "text",
    "text": "{\"content\":\"base64EncodedContent...\",\"encoding\":\"base64\"}"
  }]
}
```

### Solution Design

#### Option 1: Handle Multi-Part Content (Recommended)
Update `parseMCPResponse()` to iterate through all `content` elements and extract `resource` type:

```javascript
export function parseMCPResponse(response) {
  if (!response?.content || !Array.isArray(response.content)) {
    return [];
  }

  // Check all content elements for resource type (GitHub MCP pattern)
  for (const item of response.content) {
    if (item.type === 'resource' && item.resource) {
      // Return resource object with text content
      return {
        content: item.resource.text || '',
        uri: item.resource.uri,
        mimeType: item.resource.mimeType,
        encoding: 'utf8'
      };
    }
  }

  // Fall back to existing logic for single-element responses (GitLab pattern)
  const content = response.content[0];
  const rawData = content.text || content.data;
  // ... existing logic
}
```

#### Option 2: Add GitHub-Specific Parser
Create separate `parseGitHubMCPResponse()` function in `app/context.js:363-410`

**Recommendation**: Option 1 is better - makes parser platform-agnostic

### Implementation Plan

**Files to Modify**:
1. `app/mcp-utils.js:124-174` - Update `parseMCPResponse()`
2. `tests/unit/context.test.js` - Add test for resource-type responses

**Code Changes**:
```javascript
// app/mcp-utils.js
export function parseMCPResponse(response) {
  if (!response?.content || !Array.isArray(response.content)) {
    console.debug('MCP response missing content array');
    return [];
  }

  // GitHub MCP: Check for resource type in multi-part responses
  for (const item of response.content) {
    if (item.type === 'resource' && item.resource) {
      return {
        content: item.resource.text || '',
        uri: item.resource.uri,
        mimeType: item.resource.mimeType,
        encoding: 'utf8'
      };
    }
  }

  // GitLab MCP: Single-element text/data responses
  const content = response.content[0];
  if (!content.text && !content.data) {
    console.debug('MCP response content missing text or data field:', content);
    return [];
  }

  const rawData = content.text || content.data;

  // If already an object, return as-is
  if (typeof rawData === 'object' && rawData !== null) {
    console.debug('MCP response already parsed as object');
    return rawData;
  }

  // If string, parse as JSON or return error
  if (typeof rawData === 'string') {
    if (!rawData.trim().startsWith('{') && !rawData.trim().startsWith('[')) {
      console.debug('MCP response appears to be plain text:', rawData);
      return { error: rawData, isPlainText: true };
    }

    try {
      const parsed = JSON.parse(rawData);
      console.debug('MCP response successfully parsed from JSON string');
      return parsed;
    } catch (error) {
      console.error('Failed to parse MCP response as JSON:', error.message);
      return { error: `JSON parse failed: ${error.message}`, originalText: rawData };
    }
  }

  console.error('MCP response has unexpected type:', typeof rawData);
  return [];
}
```

### Testing Strategy

**Unit Test**:
```javascript
describe('parseMCPResponse with GitHub resource type', () => {
  it('should extract file content from resource type', () => {
    const response = {
      content: [
        { type: 'text', text: 'successfully downloaded text file' },
        {
          type: 'resource',
          resource: {
            uri: 'repo://owner/repo/contents/test.js',
            mimeType: 'text/plain; charset=utf-8',
            text: 'const foo = "bar";'
          }
        }
      ]
    };

    const result = parseMCPResponse(response);

    expect(result.content).toBe('const foo = "bar";');
    expect(result.uri).toBe('repo://owner/repo/contents/test.js');
  });
});
```

**Integration Test**:
Run `DRY_RUN=true npm start` and verify:
- ✅ No "Failed to parse content" warnings
- ✅ File contents appear in context
- ✅ Sub-agents receive actual code (not SHA messages)

---

## BUG-001-B: Claude API Rate Limiting on Large PRs

### Status
🟡 **CONFIRMED** - Solution designed, not implemented

### Severity
**Medium** - Prevents analysis on PRs with >30 files

### Symptoms
```
✗ test-analyzer failed: 429 {"type":"error","error":{"type":"rate_limit_error","message":"This request would exceed your organization's maximum usage increase rate for input tokens per minute..."}}
✗ security-analyzer failed: 429 ...
✗ performance-analyzer failed: 429 ...
✗ architecture-analyzer failed: 429 ...
```

### Root Cause Analysis

#### Problem
PR #1 has 55 files with 11,420 additions. When 4 sub-agents execute in parallel:

**Token Calculation**:
```
Context per agent = ~15,000 tokens (PR metadata + all diffs + file contents)
4 agents × 15,000 tokens = 60,000 tokens burst
```

**Claude API Limits** (Org-level):
- Input tokens per minute: ~40,000 tokens/min
- Rate limit acceleration: Gradual ramp-up required

**Result**: All 4 parallel requests hit rate limit simultaneously

#### Current Implementation
```javascript
// app/agents/SubAgentOrchestrator.js:34-70
const results = await Promise.all(
  agentTasks.map(task => this.runAgent(task.agent, state, config))
);
// ❌ All 4 agents fire simultaneously, exceeding rate limit
```

#### Evidence
- **Retry Logic**: `app/utils/retry.js:40` already handles 429 status code
- **Retry Config**: Max retries = 3, initial delay = 1000ms
- **Problem**: Retry delays insufficient for rate limit recovery (needs longer backoff)

### Solution Design

#### Immediate Solution: Adaptive Execution Mode

Add PR size detection to switch between parallel and sequential execution:

```javascript
// app/agents/SubAgentOrchestrator.js
async executeParallelAnalysis(state, config) {
  const changedFiles = state.context?.repository?.filesChanged || 0;
  const totalDiff = state.context?.statistics?.totalLines || 0;

  // Use sequential mode for large PRs
  const useSequential =
    changedFiles > config.rateLimiting.filesThreshold ||
    totalDiff > config.rateLimiting.diffLinesThreshold;

  if (useSequential) {
    console.log(`⏱️  Using sequential execution (${changedFiles} files)`);
    return await this.executeSequential(state, config);
  } else {
    console.log(`⚡ Using parallel execution (${changedFiles} files)`);
    return await this.executeParallel(state, config);
  }
}

async executeSequential(state, config) {
  const agentTasks = [...]; // Same as before

  for (const task of agentTasks) {
    try {
      const result = await this.runAgent(task.agent, state, config);
      state.findings[task.category] = result.findings;

      // Add delay between agents to respect rate limits
      if (config.rateLimiting.delayBetweenAgents > 0) {
        await new Promise(resolve =>
          setTimeout(resolve, config.rateLimiting.delayBetweenAgents)
        );
      }
    } catch (error) {
      console.warn(`  ✗ ${task.agent} failed:`, error.message);
      state.findings[task.category] = [];
    }
  }
}
```

#### Long-Term Solution: Token Budget Management

Implement token usage tracking and dynamic throttling:

```javascript
class TokenBudgetManager {
  constructor(maxTokensPerMinute) {
    this.budget = maxTokensPerMinute;
    this.used = 0;
    this.resetTime = Date.now() + 60000;
  }

  async requestTokens(estimatedTokens) {
    if (Date.now() > this.resetTime) {
      this.used = 0;
      this.resetTime = Date.now() + 60000;
    }

    if (this.used + estimatedTokens > this.budget) {
      const waitTime = this.resetTime - Date.now();
      await new Promise(resolve => setTimeout(resolve, waitTime));
      this.used = 0;
      this.resetTime = Date.now() + 60000;
    }

    this.used += estimatedTokens;
  }
}
```

### Configuration Changes

**Add to `conf/config.json`**:
```json
{
  "rateLimiting": {
    "maxTokensPerMinute": 40000,
    "filesThreshold": 30,
    "diffLinesThreshold": 5000,
    "delayBetweenAgents": 2000,
    "enableSequentialFallback": true
  }
}
```

### Implementation Plan

**Files to Modify**:
1. `app/agents/SubAgentOrchestrator.js:24-120` - Add adaptive execution
2. `conf/config.json` - Add rate limiting config
3. `app/utils/retry.js:40-110` - Increase delays for 429 errors
4. `tests/unit/sub-agent-orchestrator.test.js` - Test sequential mode

**Code Changes**:

```javascript
// app/utils/retry.js - Increase backoff for rate limits
export function isRetryableError(error) {
  // ... existing code ...

  // Special handling for rate limit errors - longer delays
  if (error.statusCode === 429 || error.message?.includes('rate_limit_error')) {
    return true; // Will use exponential backoff
  }
}

// Modify calculateBackoffDelay() to use longer delays for 429
export function calculateBackoffDelay(attempt, initialDelay = 1000, maxDelay = 30000) {
  // For rate limits, start with 5s delay instead of 1s
  const baseDelay = initialDelay * Math.pow(2, attempt);
  const jitter = Math.random() * baseDelay * 0.2;
  return Math.min(baseDelay + jitter, maxDelay);
}
```

### Testing Strategy

**Unit Tests**:
```javascript
describe('SubAgentOrchestrator sequential mode', () => {
  it('should use sequential execution for large PRs', async () => {
    const state = {
      context: { repository: { filesChanged: 50 } }
    };
    const orchestrator = new SubAgentOrchestrator(config);

    // Mock executeSequential to verify it's called
    jest.spyOn(orchestrator, 'executeSequential');

    await orchestrator.executeParallelAnalysis(state, config);

    expect(orchestrator.executeSequential).toHaveBeenCalled();
  });
});
```

**Integration Test**:
1. Configure `filesThreshold: 30` in config
2. Run `DRY_RUN=true npm start` on PR #1 (55 files)
3. Verify console shows "⏱️  Using sequential execution"
4. Verify all 4 agents complete without 429 errors
5. Verify findings > 0

---

## Action Plan

### Phase 1: Fix File Content Retrieval (Priority P0)
**Estimated Time**: 2 hours

- [ ] **Task 1.1**: Update `parseMCPResponse()` to handle resource type (30 min)
- [ ] **Task 1.2**: Add unit tests for resource-type responses (30 min)
- [ ] **Task 1.3**: Run integration test with `npm start` (15 min)
- [ ] **Task 1.4**: Verify file contents appear in logs (15 min)
- [ ] **Task 1.5**: Mark BUG-001-A as FIXED (5 min)

**Success Criteria**:
- ✅ No "Failed to parse content" warnings
- ✅ File contents visible in context logs
- ✅ `parseMCPResponse()` test coverage >90%

### Phase 2: Implement Rate Limit Protection (Priority P1)
**Estimated Time**: 3 hours

- [ ] **Task 2.1**: Add `rateLimiting` section to `conf/config.json` (15 min)
- [ ] **Task 2.2**: Implement `executeSequential()` method (45 min)
- [ ] **Task 2.3**: Add PR size detection logic (30 min)
- [ ] **Task 2.4**: Update retry delays for 429 errors (30 min)
- [ ] **Task 2.5**: Write unit tests for sequential mode (45 min)
- [ ] **Task 2.6**: Run integration test on PR #1 (15 min)
- [ ] **Task 2.7**: Mark BUG-001-B as FIXED (5 min)

**Success Criteria**:
- ✅ No 429 errors during review
- ✅ All 4 sub-agents complete successfully
- ✅ Review returns findings > 0
- ✅ Sequential execution used for PRs >30 files

### Phase 3: Documentation (Priority P2)
**Estimated Time**: 1 hour

- [ ] **Task 3.1**: Update CLAUDE.md with troubleshooting section (30 min)
- [ ] **Task 3.2**: Document rate limiting configuration (20 min)
- [ ] **Task 3.3**: Add GitHub MCP known issues section (10 min)

---

## Verification Checklist

### Pre-Fix Baseline
- [x] Confirm file content retrieval returns SHA messages
- [x] Confirm all sub-agents fail with 429 errors
- [x] Confirm review returns 0 findings

### Post-Fix Validation
- [ ] Run `npm test` - all tests pass
- [ ] Run `DRY_RUN=true npm start` on PR #1
- [ ] Verify file contents retrieved correctly
- [ ] Verify sequential execution triggered
- [ ] Verify 0 rate limit errors
- [ ] Verify findings > 0
- [ ] Verify review summary accurate

---

## Related Issues

- **Cursor Forum**: https://forum.cursor.com/t/github-mcp-get-file-contents-tool-fails-to-parse-the-response/111075
- **GitHub MCP Server**: https://github.com/github/github-mcp-server

---

## Notes

### Why This Matters
Without these fixes, the GitHub integration is **completely non-functional**:
1. No file content = No code analysis
2. Rate limits = No findings returned
3. Result: Agent approves all PRs without actual review

### Risk Assessment
- **Fix Complexity**: Low (targeted changes to 2 files)
- **Test Coverage**: High (existing patterns, easy to test)
- **Regression Risk**: Low (changes isolated to GitHub-specific code paths)

### Future Enhancements
1. **Token Budget Manager**: Implement smart throttling (Phase 3)
2. **Incremental Loading**: Fetch files on-demand vs upfront (Phase 4)
3. **Caching**: Cache file contents between review runs (Phase 4)
